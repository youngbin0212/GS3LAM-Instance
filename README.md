
# GS3LAM-Instance (Modified Version)

> ⚠**Note**:  
> This repository is a **modified version of the original [GS3LAM (ACM MM 2024)](https://github.com/lif314/GS3LAM)**.  
> The original README is preserved below for reference.  
> Here I summarize the main modifications and ongoing work.

## Modifications from the Original

### 1. 데이터/흐름 확장
- **관련 파일:** `GS3LAM.py`, `Mapper.py`  
- dataset 샘플 분기 처리 시 `inst_mask` 포함  
- keyframe 구조에 `'inst': inst_mask` 추가 -> 프레임별 인스턴스 마스크 저장  
- inst_mask는 dataset/keyframe에서 사용, inst_embed는 모델 출력 활용  

### 2. 모델 구조
- **관련 파일:** `Decoder.py`  
- `SemanticDecoder`를 2-head 구조(`sem_head`, `inst_head`)로 구현  
- 출력: `(sem_logits, inst_embed)`  
- `inst_embed`는 `F.normalize`로 L2 정규화 적용  

### 3. 손실 함수
- **관련 파일:** `Loss.py`, `contrastive_loss.py`  
- **Instance compactness loss**: 같은 인스턴스 내 임베딩을 가깝게 유지 (`losses['inst']`)  
- **Contrastive loss**: 서로 다른 인스턴스를 분리 (`losses['inst_ctr']`)
  - (config에서 `inst_ctr` 값이 0보다 크면 항상 활성화, 현재 설정은 0.05)
- `inst loss`는 **mapping 단계에서만 계산**  

### 4. 후처리 / 평가
- **관련 파일:** `instance_post.py`, `Evaluater.py`  
- `segment_instances_from_embeddings`: 임베딩 기반 Union-Find로 인스턴스 마스크 생성  
- 예측 인스턴스 마스크를 `.npy`로 저장 (GS3LAM.py 코드 블록, 평가 시 `eval()` 호출 포함)  

### 5. 학습 제어 / 최적화
- **관련 파일:** `Tracker.py`, `GaussianManager.py`, `Render.py`  
- Tracking 단계: semantic decoder `lr=0.0` (고정)  
- Mapping 단계: semantic decoder `lr=5e-4`로 학습  
- Gaussian 관리(`prune_gaussians`, `densify`)를 mapping 루프와 연동  

---

# Original README (Unmodified)




<p align="center">
  <h1 align="center">
    GS<sup>3</sup>LAM: Gaussian Semantic Splatting SLAM
    <br>
    [ACM MM 2024]
  </h1>
  <p align="center">
  <a href="https://github.com/lif314"><strong>Linfei Li</strong></a>
  ·
  <a href="https://scholar.google.com/citations?user=8VOk_S4AAAAJ&hl=en"><strong>Lin Zhang*</strong></a>
  ·
  <a href="https://scholar.google.com/citations?user=rrkp_usAAAAJ&hl=en"><strong>Zhong Wang</strong></a>
  ·
  <a href="https://scholar.google.com/citations?user=A0N_mS0AAAAJ&hl=en"><strong>Ying Shen</strong></a>
</p>

  <h3 align="center"><a href="https://github.com/lif314/GS3LAM">🌐Project page (comming soon)</a> 
  | <a href="https://dl.acm.org/doi/10.1145/3664647.3680739">📝Paper(ACM DL)</a>
  </h3>
  <div align="center"></div>
</p>

<p align="left">
  <a href="">
    <img src="./assets/teaser.png" alt="GS3LAM teaser" width="100%">
  </a>
</p>

<!-- <p align="center">
  <a href="">
    <img src="./assets/splatting_rendering.gif" alt="splatting" width="100%">
  </a>
  <a href="">
    <img src="./assets/o2_splatam_ours.gif" alt="o2_splatam_ours" width="100%">
  </a>
  <a href="">
    <img src="./assets/o3_splatam_ours.gif" alt="o3_splatam_ours" width="100%">
  </a>
  <a href="">
    <img src="./assets/r0_pointslam_ours.gif" alt="r0_pointslam_ours" width="100%">
  </a>
</p> -->

<!-- TABLE OF CONTENTS -->
<details open="open" style='padding: 10px; border-radius:5px 30px 30px 5px; border-style: solid; border-width: 1px;'>
  <summary>Table of Contents</summary>
  <ol>
    <li>
      <a href="#installation">Installation</a>
    </li>
    <li>
      <a href="#datasets">Datasets</a>
    </li>
    <li>
      <a href="#benchmarking">Benchmarking</a>
    </li>
    <li>
      <a href="#visualizer">Visualizer</a>
    </li>
    <li>
      <a href="#acknowledgement">Acknowledgement</a>
    </li>
    <li>
      <a href="#license">License</a>
    </li>
    <li>
      <a href="#citation">Citation</a>
    </li>
  </ol>
</details>

## Installation

The simplest way to install all dependences is to use [anaconda](https://www.anaconda.com/) and [pip](https://pypi.org/project/pip/) in the following steps: 

```bash
conda create -n gs3lam python==3.10
conda activate gs3lam
conda install -c "nvidia/label/cuda-11.7.0" cuda-toolkit
pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 --extra-index-url https://download.pytorch.org/whl/cu117
pip install -r requirements.txt


# install Gaussian Rasterization
pip install submodules/gaussian-semantic-rasterization
```

## Datasets

DATAROOT is `./data` by default. Please change the `basedir` path in the scene-specific config files if datasets are stored somewhere else on your machine.

### Replica

The original Replica dataset does not contain semantic labels. We obtained semantic labels from [vMAP](https://github.com/kxhit/vMAP). You can download our generated semantic Replica dataset from [here](https://huggingface.co/datasets/3David14/GS3LAM-Replica), then place the data into the `./data/Replica` folder.

> Note, if you directly use the Replica dataset provided by vMAP, please modify the [Replica Dataloader](./src/datasets/replica.py) and the  [png_depth_scale](./configs/camera/replica.yaml) parameter in config files.

### TUM-RGBD

The TUM-RGBD dataset does not have ground truth semantic labels, so it is not our evaluation dataset. However, in order to evalute the effectiveness of GS3LAM, we use pseudo-semantic labels generated by [DEVA](https://github.com/hkchengrex/Tracking-Anything-with-DEVA), which you can download from [here](https://huggingface.co/datasets/3David14/TUM-DEVA). Unfortunately, existing semantic segmentation models struggle to maintain inter-frame semantic consistency in long sequence data, so we only tested on the `freiburg1_desk` sequence.

### ScanNet

Please follow the data downloading procedure on the [ScanNet](http://www.scan-net.org/) website, and extract color/depth frames from the `.sens` file using this [code](https://github.com/ScanNet/ScanNet/blob/master/SensReader/python/reader.py).

<details>
  <summary>[Directory structure of ScanNet (click to expand)]</summary>

```
  DATAROOT
  └── scannet
        └── scene0000_00
            └── frames
                ├── color
                │   ├── 0.jpg
                │   ├── 1.jpg
                │   └── ...
                ├── depth
                │   ├── 0.png
                │   ├── 1.png
                │   └── ...
                ├── label-filt
                │   ├── 0.png
                │   ├── 1.png
                │   └── ...
                ├── intrinsic
                └── pose
                    ├── 0.txt
                    ├── 1.txt
                    └── ...
```
</details>


We use the following sequences: 
```
scene0000_00
scene0059_00
scene0106_00
scene0169_00
scene0181_00
scene0207_00
```

## Benchmarking
### TUM-RGBD

To run GS3LAM on the `freiburg1_desk` scene, run the following command:

```bash
python run.py configs/Tum/tum_fr1.py
```

### Replica

To run GS3LAM on the `office0` scene, run the following command:

```bash
python run.py configs/Replica/office0.py
```

To run GS3LAM on all Replica scenes, run the following command:

```bash
bash scripts/eval_full_replica.sh
```

### ScanNet

To run GS3LAM on the `scene0059_00` scene, run the following command:

```bash
python run.py configs/Scannet/scene0059_00.py
```

To run GS3LAM on all ScanNet scenes, run the following command:

```bash
bash scripts/eval_full_scannet.bash
```

## Visualizer

### Online/Offline Reconstruction and Export Mesh

- Define the ``SEED`` and ``SCENE_NUM`` environment variables in the configuration file.
```bash
# ``SEED`` is the random seed used during training, which should be consistent with the configuration.
export SEED=1

# ``SCENE_NUM`` is the index of the data sequence in the following list.
# Replica: ["room0", "room1", "room2","office0", "office1", "office2", "office3", "office4"]
# Scannet: ["scene0059_00", "scene0106_00", "scene0169_00", "scene0181_00", "scene0207_00", "scene0000_00"]
export SCENE_NUM=0
```

- Online reconstruction.
```bash
# optional mode: [color, depth, centers, sem, sem_color, sem_feature]
python visualizer/online_recon.py --mode color --logdir path/to/the/log
```

- Offline reconstruction.
```bash
# optional mode: [color, depth, centers, sem, sem_color, sem_feature]
python visualizer/offline_recon.py --mode sem_color --logdir path/to/the/log
```

- Export Mesh
```bash
# optional mode: [color, sem]
python visualizer/export_mesh.py --mode color --logdir path/to/the/log
```

### Plot Optimization Bias

To draw ``Fig. 2`` in the paper, which can demonstrate the relationship between optimization iterations, rendering quality and camera trajectories.

```bash
python visualizer/plot_opt_bias.py --logdir path/to/the/log
```

## Acknowledgement
We thank the authors of the following repositories for their open-source code:

- [3D Gaussian Splatting](https://github.com/graphdeco-inria/gaussian-splatting)
- [SplaTAM](https://github.com/spla-tam)
- [Gaussian-SLAM](https://github.com/VladimirYugay/Gaussian-SLAM)
- [vMAP](https://github.com/kxhit/vMAP)
- [Point-SLAM](https://github.com/eriksandstroem/Point-SLAM)
- [Gaussian Grouping](https://github.com/lkeab/gaussian-grouping)

## License
As our work heavily relies on [SplaTAM](https://github.com/spla-tam), we kindly ask that you adhere to the guidelines set forth in SplaTAM's [LICENSE](https://github.com/spla-tam/SplaTAM/blob/main/LICENSE).

## Citation

If you find our paper and code useful for your research, please use the following BibTeX entry.

```bibtex
@inproceedings{li2024gs3lam,
      author = {Li, Linfei and Zhang, Lin and Wang, Zhong and Shen, Ying},
      title = {GS3LAM: Gaussian Semantic Splatting SLAM},
      year = {2024},
      publisher = {Association for Computing Machinery},
      address = {New York, NY, USA},
      booktitle = {Proceedings of the 32nd ACM International Conference on Multimedia},
      pages = {3019–3027},
      numpages = {9},
      location = {Melbourne VIC, Australia},
      series = {MM '24}
}
```
